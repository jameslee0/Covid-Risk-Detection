{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "face_mask_detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1pIVnME09fXP1WLOyD5uSKboWzKY1vxU9",
      "authorship_tag": "ABX9TyPBC8NAZy8vCB96VRTsEnWP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/HectorENevarez/guidelines_detection/blob/main/Training_Model/face_mask_detection.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hePnpJX-Hxa-"
      },
      "source": [
        "# Face Mask Detection Model\r\n",
        "This ipynb goes over the steps taken to train the face detection model. <br>\r\n",
        "This model is an adaptation of [Google's BigTransfer(BiT)](https://colab.research.google.com/github/google-research/big_transfer/blob/master/colabs/big_transfer_tf2.ipynb)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVINKZLWpiXo"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "import tensorflow_hub as hub\r\n",
        "\r\n",
        "import tensorflow_datasets as tfds\r\n",
        "\r\n",
        "import time\r\n",
        "\r\n",
        "from PIL import Image\r\n",
        "import requests\r\n",
        "from io import BytesIO\r\n",
        "\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "import os\r\n",
        "import pathlib\r\n",
        "import cv2\r\n",
        "from google.colab.patches import cv2_imshow"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PluQcRzVqvdK",
        "outputId": "539a5a18-f44e-4450-94f4-5e093bbbd32e"
      },
      "source": [
        "!wget https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\r\n",
        "\r\n",
        "imagenet_int_to_str = {}\r\n",
        "\r\n",
        "with open('ilsvrc2012_wordnet_lemmas.txt', 'r') as f:\r\n",
        "  for i in range(1000):\r\n",
        "    row = f.readline()\r\n",
        "    row = row.rstrip()\r\n",
        "    imagenet_int_to_str.update({i: row})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-15 00:57:05--  https://storage.googleapis.com/bit_models/ilsvrc2012_wordnet_lemmas.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 172.253.62.128, 172.217.7.208, 142.250.31.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|172.253.62.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 21675 (21K) [text/plain]\n",
            "Saving to: ‘ilsvrc2012_wordnet_lemmas.txt.3’\n",
            "\n",
            "\r          ilsvrc201   0%[                    ]       0  --.-KB/s               \rilsvrc2012_wordnet_ 100%[===================>]  21.17K  --.-KB/s    in 0s      \n",
            "\n",
            "2021-01-15 00:57:06 (88.5 MB/s) - ‘ilsvrc2012_wordnet_lemmas.txt.3’ saved [21675/21675]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rJCS032tqviD"
      },
      "source": [
        "tf_labels = ['No Mask', 'Mask', 'Unkown']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gqWy8p4cqvoH"
      },
      "source": [
        "model_url = \"https://tfhub.dev/google/bit/m-r50x1/1\"\r\n",
        "module = hub.KerasLayer(model_url)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oFJzD7sSIbxI"
      },
      "source": [
        "## Creating the dataset\r\n",
        "The following code was used in order to create the dataset of face masks. Each cell is for each class. In this case there would be three cells; Mask, no mask, unknown"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vA0Rk5v_sJMH"
      },
      "source": [
        "# Face Folder\r\n",
        "\r\n",
        "face_img_loc = './drive/MyDrive/alt-faces-or-masks/faces' #Insert your path here\r\n",
        "counter = 0\r\n",
        "labels = []\r\n",
        "mask_data = []\r\n",
        "for img in os.listdir(face_img_loc):\r\n",
        "  pic = cv2.imread(os.path.join(face_img_loc,img))\r\n",
        "  pic = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\r\n",
        "  pic = cv2.resize(pic,(32,32))\r\n",
        "  mask_data.append(pic)\r\n",
        "  labels.append(0) #Face"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdUUvNuwwjQ5"
      },
      "source": [
        "#Mask Folder\r\n",
        "\r\n",
        "mask_img_loc = './drive/MyDrive/alt-faces-or-masks/masks' # Insert path here\r\n",
        "counter = 0\r\n",
        "for img in os.listdir(mask_img_loc):\r\n",
        "    pic = cv2.imread(os.path.join(mask_img_loc,img))\r\n",
        "    pic = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\r\n",
        "    pic = cv2.resize(pic,(32,32))\r\n",
        "    mask_data.append(pic)\r\n",
        "    labels.append(1) #Mask"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uW01BMPvYpbk"
      },
      "source": [
        "# Unknown folder\r\n",
        "\r\n",
        "rec_img_loc = './drive/MyDrive/alt-faces-or-masks/reclassify' #Insert path here\r\n",
        "\r\n",
        "for img in os.listdir(rec_img_loc):\r\n",
        "    pic = cv2.imread(os.path.join(rec_img_loc,img))\r\n",
        "    pic = cv2.cvtColor(pic,cv2.COLOR_BGR2RGB)\r\n",
        "    pic = cv2.resize(pic,(32,32))\r\n",
        "    mask_data.append(pic)\r\n",
        "    labels.append(2) #Unkown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qk6JH25I0R6y",
        "outputId": "5faf3ca2-54db-4d7c-cede-5f555eadc493"
      },
      "source": [
        "len(mask_data) # Number of images in dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2999"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH3zRERQ4kTv"
      },
      "source": [
        "#Shuffle dataset randomly\r\n",
        "\r\n",
        "from sklearn.utils import shuffle\r\n",
        "\r\n",
        "X, y = shuffle(mask_data, labels, random_state=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FxA1av5N80IA"
      },
      "source": [
        "y = np.array(y).astype('int64')\r\n",
        "\r\n",
        "ds = tf.data.Dataset.from_tensor_slices(\r\n",
        "    {\"image\" : X,\r\n",
        "     \"label\" : y}) # Create data set"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zh5LLeZpCxRT",
        "outputId": "d89e0d9c-9ef3-45c9-8103-45fb8dfa578c"
      },
      "source": [
        "ds"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<TensorSliceDataset shapes: {image: (32, 32, 3), label: ()}, types: {image: tf.uint8, label: tf.int64}>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DNomLasp0k2l"
      },
      "source": [
        "# Seperation for train test split\r\n",
        "\r\n",
        "train_split = 0.9\r\n",
        "num_examples = 2999\r\n",
        "num_train = int(train_split * num_examples)\r\n",
        "\r\n",
        "ds_train = ds.take(num_train)\r\n",
        "ds_test = ds.skip(num_train)\r\n",
        "\r\n",
        "DATASET_NUM_TRAIN_EXAMPLES = num_examples"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvuE0Omw6pt-",
        "outputId": "2fc6b20b-b2fc-4cd9-f6e3-bd85207bcc9a"
      },
      "source": [
        "print(\"The dataset has {} training samples and {} testing sample\".format(len(ds_train), len(ds_test)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The dataset has 2699 training samples and 300 testing sample\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "khxDSpha3mD7"
      },
      "source": [
        "class MyBiTModel(tf.keras.Model):\r\n",
        "  \"\"\"BiT with a new head.\"\"\"\r\n",
        "\r\n",
        "  def __init__(self, num_classes, module):\r\n",
        "    super().__init__()\r\n",
        "\r\n",
        "    self.num_classes = num_classes\r\n",
        "    self.head = tf.keras.layers.Dense(num_classes, kernel_initializer='zeros')\r\n",
        "    self.bit_model = module\r\n",
        "  \r\n",
        "  def call(self, images):\r\n",
        "    # No need to cut head off since we are using feature extractor model\r\n",
        "    bit_embedding = self.bit_model(images)\r\n",
        "    return self.head(bit_embedding)\r\n",
        "\r\n",
        "model = MyBiTModel(num_classes=3, module=module)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YdE8xg7YhH65"
      },
      "source": [
        "IMAGE_SIZE = \"=\\u003C96x96 px\" #@param [\"=<96x96 px\",\"> 96 x 96 px\"]\r\n",
        "DATASET_SIZE = \"\\u003C20k examples\" #@param [\"<20k examples\", \"20k-500k examples\", \">500k examples\"]\r\n",
        "\r\n",
        "if IMAGE_SIZE == \"=<96x96 px\":\r\n",
        "  RESIZE_TO = 160\r\n",
        "  CROP_TO = 128\r\n",
        "else:\r\n",
        "  RESIZE_TO = 512\r\n",
        "  CROP_TO = 480\r\n",
        "\r\n",
        "if DATASET_SIZE == \"<20k examples\":\r\n",
        "  SCHEDULE_LENGTH = 500\r\n",
        "  SCHEDULE_BOUNDARIES = [200, 300, 400]\r\n",
        "elif DATASET_SIZE == \"20k-500k examples\":\r\n",
        "  SCHEDULE_LENGTH = 10000\r\n",
        "  SCHEDULE_BOUNDARIES = [3000, 6000, 9000]\r\n",
        "else:\r\n",
        "  SCHEDULE_LENGTH = 20000\r\n",
        "  SCHEDULE_BOUNDARIES = [6000, 12000, 18000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qM2AZoqI3mFp"
      },
      "source": [
        "# Preprocessing helper functions\r\n",
        "\r\n",
        "# Create data pipelines for training and testing:\r\n",
        "BATCH_SIZE = 512\r\n",
        "SCHEDULE_LENGTH = SCHEDULE_LENGTH * 512 / BATCH_SIZE\r\n",
        "\r\n",
        "STEPS_PER_EPOCH = 10\r\n",
        "\r\n",
        "def cast_to_tuple(features):\r\n",
        "  return (features['image'], features['label'])\r\n",
        "  \r\n",
        "def preprocess_train(features):\r\n",
        "  # Apply random crops and horizontal flips for all tasks \r\n",
        "  # except those for which cropping or flipping destroys the label semantics\r\n",
        "  # (e.g. predict orientation of an object)\r\n",
        "  features['image'] = tf.image.random_flip_left_right(features['image'])\r\n",
        "  features['image'] = tf.image.resize(features['image'], [RESIZE_TO, RESIZE_TO])\r\n",
        "  features['image'] = tf.image.random_crop(features['image'], [CROP_TO, CROP_TO, 3])\r\n",
        "  features['image'] = tf.cast(features['image'], tf.float32) / 255.0\r\n",
        "  return features\r\n",
        "\r\n",
        "def preprocess_test(features):\r\n",
        "  features['image'] = tf.image.resize(features['image'], [RESIZE_TO, RESIZE_TO])\r\n",
        "  features['image'] = tf.cast(features['image'], tf.float32) / 255.0\r\n",
        "  return features\r\n",
        "\r\n",
        "pipeline_train = (ds_train\r\n",
        "                  .shuffle(10000)\r\n",
        "                  .repeat(int(SCHEDULE_LENGTH * BATCH_SIZE / DATASET_NUM_TRAIN_EXAMPLES * STEPS_PER_EPOCH) + 1 + 50)  # repeat dataset_size / num_steps\r\n",
        "                  .map(preprocess_train, num_parallel_calls=8)\r\n",
        "                  .batch(BATCH_SIZE)\r\n",
        "                  .map(cast_to_tuple)  # for keras model.fit\r\n",
        "                  .prefetch(2))\r\n",
        "\r\n",
        "pipeline_test = (ds_test.map(preprocess_test, num_parallel_calls=1)\r\n",
        "                  .map(cast_to_tuple)  # for keras model.fit\r\n",
        "                  .batch(BATCH_SIZE)\r\n",
        "                  .prefetch(2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6eq-tYq36Yul",
        "outputId": "f69cc651-f731-4f41-a821-f62a63109464"
      },
      "source": [
        "pipeline_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<PrefetchDataset shapes: ((None, 128, 128, 3), (None,)), types: (tf.float32, tf.int64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6wxTJZD3mIH"
      },
      "source": [
        "# Define optimiser and loss\r\n",
        "\r\n",
        "lr = 0.003 * BATCH_SIZE / 512 \r\n",
        "\r\n",
        "# Decay learning rate by a factor of 10 at SCHEDULE_BOUNDARIES.\r\n",
        "lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(boundaries=SCHEDULE_BOUNDARIES, \r\n",
        "                                                                   values=[lr, lr*0.1, lr*0.001, lr*0.0001])\r\n",
        "optimizer = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9)\r\n",
        "\r\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cadvhyca3mKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2909b8cc-4ad1-44f4-c31a-779144f79efc"
      },
      "source": [
        "model.compile(optimizer=optimizer,\r\n",
        "              loss=loss_fn,\r\n",
        "              metrics=['accuracy'])\r\n",
        "\r\n",
        "# Fine-tune model\r\n",
        "history = model.fit(\r\n",
        "    pipeline_train,\r\n",
        "    batch_size=BATCH_SIZE,\r\n",
        "    steps_per_epoch=STEPS_PER_EPOCH,\r\n",
        "    epochs=50,\r\n",
        "    validation_data=pipeline_test\r\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "10/10 [==============================] - 30s 1s/step - loss: 2.0419 - accuracy: 0.4688 - val_loss: 3.9154 - val_accuracy: 0.6267\n",
            "Epoch 2/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 1.7922 - accuracy: 0.7799 - val_loss: 2.4480 - val_accuracy: 0.7800\n",
            "Epoch 3/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 1.1809 - accuracy: 0.8507 - val_loss: 1.5704 - val_accuracy: 0.8033\n",
            "Epoch 4/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.8674 - accuracy: 0.8758 - val_loss: 1.2902 - val_accuracy: 0.8233\n",
            "Epoch 5/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.5919 - accuracy: 0.8889 - val_loss: 1.0184 - val_accuracy: 0.8333\n",
            "Epoch 6/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.5319 - accuracy: 0.8864 - val_loss: 0.9110 - val_accuracy: 0.8267\n",
            "Epoch 7/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.4647 - accuracy: 0.8964 - val_loss: 0.9101 - val_accuracy: 0.8133\n",
            "Epoch 8/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.4001 - accuracy: 0.8982 - val_loss: 0.8350 - val_accuracy: 0.8300\n",
            "Epoch 9/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.3570 - accuracy: 0.9038 - val_loss: 0.7643 - val_accuracy: 0.8233\n",
            "Epoch 10/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.3311 - accuracy: 0.9063 - val_loss: 0.6670 - val_accuracy: 0.8333\n",
            "Epoch 11/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2999 - accuracy: 0.9121 - val_loss: 0.6145 - val_accuracy: 0.8400\n",
            "Epoch 12/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2290 - accuracy: 0.9268 - val_loss: 0.6305 - val_accuracy: 0.8400\n",
            "Epoch 13/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2260 - accuracy: 0.9230 - val_loss: 0.6092 - val_accuracy: 0.8300\n",
            "Epoch 14/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2124 - accuracy: 0.9286 - val_loss: 0.5444 - val_accuracy: 0.8500\n",
            "Epoch 15/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2105 - accuracy: 0.9296 - val_loss: 0.5219 - val_accuracy: 0.8333\n",
            "Epoch 16/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1861 - accuracy: 0.9374 - val_loss: 0.5322 - val_accuracy: 0.8367\n",
            "Epoch 17/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1839 - accuracy: 0.9338 - val_loss: 0.5219 - val_accuracy: 0.8333\n",
            "Epoch 18/50\n",
            "10/10 [==============================] - 11s 1s/step - loss: 0.1563 - accuracy: 0.9447 - val_loss: 0.4951 - val_accuracy: 0.8300\n",
            "Epoch 19/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1678 - accuracy: 0.9370 - val_loss: 0.4838 - val_accuracy: 0.8367\n",
            "Epoch 20/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1731 - accuracy: 0.9326 - val_loss: 0.4859 - val_accuracy: 0.8100\n",
            "Epoch 21/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.2532 - accuracy: 0.9123 - val_loss: 0.5685 - val_accuracy: 0.8333\n",
            "Epoch 22/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1805 - accuracy: 0.9346 - val_loss: 0.5141 - val_accuracy: 0.8167\n",
            "Epoch 23/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1681 - accuracy: 0.9366 - val_loss: 0.4856 - val_accuracy: 0.8467\n",
            "Epoch 24/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1549 - accuracy: 0.9426 - val_loss: 0.4641 - val_accuracy: 0.8300\n",
            "Epoch 25/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1367 - accuracy: 0.9459 - val_loss: 0.4569 - val_accuracy: 0.8433\n",
            "Epoch 26/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1314 - accuracy: 0.9548 - val_loss: 0.4565 - val_accuracy: 0.8500\n",
            "Epoch 27/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1332 - accuracy: 0.9508 - val_loss: 0.4568 - val_accuracy: 0.8367\n",
            "Epoch 28/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1459 - accuracy: 0.9462 - val_loss: 0.4594 - val_accuracy: 0.8433\n",
            "Epoch 29/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1401 - accuracy: 0.9472 - val_loss: 0.4558 - val_accuracy: 0.8400\n",
            "Epoch 30/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1446 - accuracy: 0.9474 - val_loss: 0.4579 - val_accuracy: 0.8467\n",
            "Epoch 31/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1383 - accuracy: 0.9500 - val_loss: 0.4646 - val_accuracy: 0.8333\n",
            "Epoch 32/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1494 - accuracy: 0.9495 - val_loss: 0.4746 - val_accuracy: 0.8267\n",
            "Epoch 33/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1494 - accuracy: 0.9424 - val_loss: 0.4754 - val_accuracy: 0.8267\n",
            "Epoch 34/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1592 - accuracy: 0.9385 - val_loss: 0.4723 - val_accuracy: 0.8267\n",
            "Epoch 35/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1439 - accuracy: 0.9468 - val_loss: 0.4687 - val_accuracy: 0.8267\n",
            "Epoch 36/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1367 - accuracy: 0.9476 - val_loss: 0.4654 - val_accuracy: 0.8333\n",
            "Epoch 37/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1404 - accuracy: 0.9500 - val_loss: 0.4630 - val_accuracy: 0.8333\n",
            "Epoch 38/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1320 - accuracy: 0.9540 - val_loss: 0.4611 - val_accuracy: 0.8333\n",
            "Epoch 39/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1383 - accuracy: 0.9479 - val_loss: 0.4594 - val_accuracy: 0.8300\n",
            "Epoch 40/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1546 - accuracy: 0.9442 - val_loss: 0.4582 - val_accuracy: 0.8300\n",
            "Epoch 41/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1484 - accuracy: 0.9491 - val_loss: 0.4575 - val_accuracy: 0.8333\n",
            "Epoch 42/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1486 - accuracy: 0.9415 - val_loss: 0.4573 - val_accuracy: 0.8367\n",
            "Epoch 43/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1441 - accuracy: 0.9503 - val_loss: 0.4572 - val_accuracy: 0.8367\n",
            "Epoch 44/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1328 - accuracy: 0.9530 - val_loss: 0.4571 - val_accuracy: 0.8367\n",
            "Epoch 45/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1557 - accuracy: 0.9413 - val_loss: 0.4570 - val_accuracy: 0.8367\n",
            "Epoch 46/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1364 - accuracy: 0.9478 - val_loss: 0.4570 - val_accuracy: 0.8367\n",
            "Epoch 47/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1288 - accuracy: 0.9540 - val_loss: 0.4569 - val_accuracy: 0.8367\n",
            "Epoch 48/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1324 - accuracy: 0.9510 - val_loss: 0.4569 - val_accuracy: 0.8367\n",
            "Epoch 49/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1409 - accuracy: 0.9491 - val_loss: 0.4568 - val_accuracy: 0.8367\n",
            "Epoch 50/50\n",
            "10/10 [==============================] - 10s 1s/step - loss: 0.1498 - accuracy: 0.9431 - val_loss: 0.4568 - val_accuracy: 0.8367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m3rfgdp7R9xB",
        "outputId": "e639746b-9f4a-4f62-b413-0d58a2039b89"
      },
      "source": [
        "model.save('./drive/MyDrive/mask_Detect')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/mask_Detect/assets\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: ./drive/MyDrive/mask_Detect/assets\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}